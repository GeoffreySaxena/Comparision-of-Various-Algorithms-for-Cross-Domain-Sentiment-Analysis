{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAJOR 1: Comparision of Various Algorithms for Cross Domain Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1vDf5FuWwfzUqRKtwvuaY6RTLDC1e9vb5",
      "authorship_tag": "ABX9TyMLVm+uS1PexycBVcZkqyXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeoffreySaxena/Comparision-of-Various-Algorithms-for-Cross-Domain-Sentiment-Analysis/blob/main/MAJOR_1_Comparision_of_Various_Algorithms_for_Cross_Domain_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSwA_TWWksD",
        "outputId": "73beecbd-b6d1-4f4f-8704-3ac9900de676"
      },
      "source": [
        "%cd /content/drive/MyDrive/Cross-domain-sentiment-analysis-master/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Cross-domain-sentiment-analysis-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIigqAwhzHQB",
        "outputId": "f5b83929-61a7-40ba-ab4e-c85fe049d24c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1hgNaYwaKa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt  \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score,confusion_matrix\n",
        "from sklearn.feature_selection import SelectKBest,chi2\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbBLfeD5XOaw"
      },
      "source": [
        "train_list = [\"Dataset/Actualdata/Books/Bookstrain.txt\",\"Dataset/Actualdata/Dvd/Dvdtrain.txt\",\"Dataset/Actualdata/Electronics/Electronicstrain.txt\",\"Dataset/Actualdata/Kitchen/Kitchentrain.txt\"]\n",
        "\n",
        "test_list = [\"Dataset/Actualdata/Books/Bookstest.txt\",\"Dataset/Actualdata/Dvd/Dvdtest.txt\",\"Dataset/Actualdata/Electronics/Electronicstest.txt\",\"Dataset/Actualdata/Kitchen/Kitchentest.txt\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fS5-jbuwfRq"
      },
      "source": [
        "stopword = stopwords.words('english') "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL5Edwmvwisw"
      },
      "source": [
        "def preprocess(sentence):\n",
        "\tsentence = re.sub('[^\\w\\s]',\" \",str(sentence))\n",
        "\tsentence = re.sub('[^a-zA-Z]',\" \",str(sentence))\n",
        "\tsents = word_tokenize(sentence)\n",
        "\tnew_sents = \" \"\n",
        "\tfor i in range(len(sents)):\n",
        "\t\tif(sents[i].lower() not in stopword):\n",
        "\t\t\tnew_sents+=sents[i].lower()+\" \"\n",
        "\treturn new_sents"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdBUhMhuwlaZ"
      },
      "source": [
        "def preprocess_test(choice):\n",
        "\n",
        "\tthe_file = test_list[choice]\n",
        "\t#print(the_file)\n",
        "\twith open(the_file,'r',encoding='utf-8') as f:\n",
        "\t\ttest_data = f.readlines()\n",
        "\n",
        "\tcorpus_test = []\n",
        "\n",
        "\tfor i in range(400):\n",
        "\t\tsent = test_data[i]\n",
        "\t\tsent = sent[0:len(sent)-1]\n",
        "\t\tcorpus_test.append(preprocess(sent))\n",
        "\n",
        "\t#print(corpus_test[0])\n",
        "\n",
        "\tlabel_test = np.zeros(400)\n",
        "\tlabel_test[0:200] = 1\n",
        "\n",
        "\n",
        "\treturn [corpus_test,label_test]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydQD_fx6wnvt"
      },
      "source": [
        "def preprocess_train(choice):\n",
        "\n",
        "\n",
        "\tthe_file = train_list[choice]\n",
        "\t#print(the_file)\n",
        "\twith open(the_file,'r',encoding='utf-8') as f:\n",
        "\t\ttrain_data = f.readlines()\n",
        "\n",
        "\n",
        "\tcorpus_train = []\n",
        "\n",
        "\tfor i in range(1600):\n",
        "\t\tsent = train_data[i]\n",
        "\t\tsent = sent[0:len(sent)-1]\n",
        "\t\tcorpus_train.append(preprocess(sent))\n",
        "\n",
        "\t#print(corpus_train[0])\n",
        "\n",
        "\tlabel_train = np.zeros(1600)\n",
        "\tlabel_train[0:800] = 1\n",
        "\n",
        "\treturn [corpus_train,label_train]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXIRQL2Ewp7d"
      },
      "source": [
        "def preprocessing(train_choice,test_choice):\n",
        "\n",
        "\tcorpus_train,label_train = preprocess_train(train_choice)\n",
        "\n",
        "\tcorpus_test,label_test = preprocess_test(test_choice)\n",
        "\n",
        "\treturn corpus_train,label_train,corpus_test,label_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYA0FnORtvCO"
      },
      "source": [
        "def extract_files(pos_file,neg_file,num):\n",
        "\t\n",
        "\twith open(pos_file,'r',encoding='utf-8') as f:\n",
        "\t\tpos = f.readlines()\n",
        "\twith open(neg_file,'r',encoding='utf-8') as f:\n",
        "\t\tneg = f.readlines()\n",
        "\n",
        "\n",
        "\tcorpus_pos = []\n",
        "\tflag = False\n",
        "\n",
        "\tfor i in range(len(pos)):\n",
        "\t\tif(pos[i]==\"<review_text>\\n\"):\n",
        "\t\t\tflag = True\n",
        "\t\t\tnew_str = \" \"\n",
        "\t\t\tcontinue\n",
        "\t\telif(pos[i]==\"</review_text>\\n\"):\n",
        "\t\t\tflag = False\n",
        "\t\t\tcorpus_pos.append(new_str)\n",
        "\t\t\tcontinue\n",
        "\t\tif(flag):\n",
        "\t\t\tif(len(pos[i])>1):\n",
        "\t\t\t\tsent = pos[i]\n",
        "\t\t\t\tsent = sent[0:len(sent)-1]\n",
        "\t\t\t\tif(sent[0]=='\\t'):\n",
        "\t\t\t\t\tsent = sent[1:len(sent)-1]\n",
        "\t\t\tnew_str+=sent  \n",
        "\t\n",
        "\n",
        "\tcorpus_neg = []\n",
        "\tflag = False\n",
        "\n",
        "\n",
        "\tfor i in range(len(neg)):\n",
        "\t\tif(neg[i]==\"<review_text>\\n\"):\n",
        "\t\t\tflag = True\n",
        "\t\t\tnew_str = \" \"\n",
        "\t\t\tcontinue\n",
        "\t\telif(neg[i]==\"</review_text>\\n\"):\n",
        "\t\t\tflag = False\n",
        "\t\t\tcorpus_neg.append(new_str)\n",
        "\t\t\tcontinue\n",
        "\t\tif(flag):\n",
        "\t\t\tif(len(neg[i])>1):\n",
        "\t\t\t\tsent = neg[i]\n",
        "\t\t\t\tsent = sent[0:len(sent)-1]\n",
        "\t\t\t\tif(sent[0]=='\\t'):\n",
        "\t\t\t\t\tsent = sent[1:len(sent)-1]\n",
        "\t\t\tnew_str+=sent\n",
        "\n",
        "\t\n",
        "\ttrain = []\n",
        "\n",
        "\tfor i in range(800):\n",
        "\t\ttrain.append(corpus_pos[i])\n",
        "\n",
        "\tfor i in range(800):\n",
        "\t\ttrain.append(corpus_neg[i])\n",
        "\n",
        "\n",
        "\ttest = []\n",
        "\n",
        "\tfor i in range(800,1000):\n",
        "\t\ttest.append(corpus_pos[i])\n",
        "\n",
        "\tfor i in range(800,1000):\n",
        "\t\ttest.append(corpus_neg[i])\n",
        "\n",
        "\t\n",
        "\tdir_to_write =[\"Books\",\"Dvd\",\"Electronics\",\"Kitchen\"]\n",
        "\toriginal_dir = \"Dataset/Actualdata\"+\"/\"+dir_to_write[num]+\"/\"\n",
        "\n",
        "\ttrain_file = original_dir+dir_to_write[num]+\"train.txt\"\n",
        "\ttest_file = original_dir+dir_to_write[num]+\"test.txt\"\n",
        "\n",
        "\n",
        "\t\n",
        "\twith open(train_file,'w',encoding='utf-8') as f:\n",
        "\t\tfor i in range(len(train)):\n",
        "\t\t\tf.write(train[i])\n",
        "\t\t\tf.write(\"\\n\")\n",
        "\n",
        "\n",
        "\twith open(test_file,'w',encoding='utf-8') as f:\n",
        "\t\tfor i in range(len(test)):\n",
        "\t\t\tf.write(test[i])\n",
        "\t\t\tf.write(\"\\n\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDlvtsfatu_q"
      },
      "source": [
        "vector_parameters = [[2,0.8],[3,0.8],[3,0.8],[3,0.8],[3,0.8],[3,0.8],[3,0.8],[3,0.8],[3,0.8],\n",
        "[3,0.8],[3,0.8],[3,0.8]]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW0wFt3zw1Nb"
      },
      "source": [
        "def featureextraction(train_corpus,test_corpus,label_train,train_choice,test_choice):\n",
        "\n",
        "\tval = (train_choice)*3 + test_choice\n",
        "\n",
        "\tparam  = vector_parameters[val]\n",
        "\tmindf = param[0]\n",
        "\tmaxdf = param[1]\n",
        "\n",
        "\tvectorizer = TfidfVectorizer(min_df=mindf,max_df=maxdf,use_idf=True,sublinear_tf=True,stop_words='english')\n",
        "\n",
        "\ttrain_corpus_tf_idf = vectorizer.fit_transform(train_corpus,label_train)\n",
        "\n",
        "\ttest_corpus_tf_idf = vectorizer.transform(test_corpus)\n",
        "\n",
        "\treturn [train_corpus_tf_idf,test_corpus_tf_idf]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MVmSfLgtu8v"
      },
      "source": [
        "chi_square_parameters = [[5000],[4000],[4000],[4000],[4000],[4000],[4000],['all'],['all'],['all'],\n",
        "['all'],['all'],[2500],['all']]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhTc5DV-w7Cm"
      },
      "source": [
        "def featureselection(train_corpus_tf_idf,test_corpus_tf_idf,label_train,train_choice,test_choice):\n",
        "\n",
        "\tval = (train_choice)*3 + test_choice\n",
        "\n",
        "\tk = chi_square_parameters[val][0]\n",
        "\n",
        "\tif(k=='all'):\n",
        "\t\tK = train_corpus_tf_idf.shape[1]\n",
        "\telse:\n",
        "\t\tK = k \n",
        "\n",
        "\tvectorizer_chi2 = SelectKBest(chi2,k=K)\n",
        "\n",
        "\tchi_train_corpus_tf_idf = vectorizer_chi2.fit_transform(train_corpus_tf_idf,label_train)\n",
        "\n",
        "\tchi_test_corpus_tf_idf = vectorizer_chi2.transform(test_corpus_tf_idf)\n",
        "\n",
        "\treturn [chi_train_corpus_tf_idf,chi_test_corpus_tf_idf]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w-BCxcztu5-"
      },
      "source": [
        "lr_parameters = [[7,0.5],[8,0.5],[6,0.5],[10,0.8],[8,0.6],[14,0.9],[7,0.8],[8,0.8],[3,0.8],[5,0.7],[2,0.5],[5,0.9]]\n",
        "rbf_parameters = [[8,0.8],[9,0.6],[13,0.7],[5,0.6],[7,0.5],[5,0.6],[5,0.6],[12,0.8],[2,0.6],[5,0.8],[8,0.8],[5,0.8]]\n",
        "dt_parameters = [[8,0.8],[9,0.5],[15,0.5],[15,0.9],[8,0.9],[10,0.7],[15,0.9],[25,0.8],[15,0.9],[5,0.9],[15,0.9],[15,0.9]]\n",
        "linear_parameters = [[8,0.5],[10,0.5],[7,0.9],[10,0.5],[13,0.6],[5,0.5],[7,0.8],[9,0.9],[3,0.4],[5,0.8],[5,0.9],[5,0.8]]\n",
        "nb_parameters = [[8,0.5],[3,0.6],[5,0.7],[2,0.5],[5,0.9],[5,0.9],[3,0.9],[8,0.8],[5,0.9],[11,0.6],[3,0.9],[3,0.9]]\n",
        "knn_parameters = [[7,0.5],[7,0.6],[7,0.5],[9,0.6],[15,0.8],[15,0.9],[10,0.8],[10,0.8],[15,0.9],[5,0.8],[10,0.9],[10,0.8]]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNZlBp0exBxN"
      },
      "source": [
        "def bagging_classify_sentiment(classifier,chi_train_corpus_tf_idf,chi_test_corpus_tf_idf,label_train,label_test,num,samples):\n",
        "    \n",
        "    clf   = BaggingClassifier(base_estimator=classifier,random_state=0,max_samples=samples,n_estimators=num)\n",
        "    clf.fit(chi_train_corpus_tf_idf,label_train)\n",
        "    pred = clf.predict(chi_test_corpus_tf_idf)\n",
        "    accuracy = clf.score(chi_test_corpus_tf_idf,label_test)\n",
        "    cm = confusion_matrix(pred,label_test)\n",
        "    f1 = f1_score(pred,label_test)\n",
        "    return accuracy,f1,cm "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdHoLn5cxETa"
      },
      "source": [
        "def bagging_train(chi_train_corpus_tf_idf,label_train,chi_test_corpus_tf_idf,label_test,train_choice,test_choice):\n",
        "\n",
        "\tx = (train_choice)*3 + test_choice\n",
        "\n",
        "\trbf_function = [[0.9],[0.9],[0.9],[0.9],[0.9],[0.8],[0.9],[0.9],[0.8],[0.9],[0.9],[0.9]]\n",
        "\n",
        "\tGamma = rbf_function[x][0]\n",
        "\n",
        "\tclassify = [\"LR\",\"SVM-RBF\",\"DT\",\"SVM-L\",\"MNB\",\"KNN\"]\n",
        "\n",
        "\n",
        "\n",
        "\tclassifiers = [LogisticRegression(random_state=0),SVC(gamma=Gamma),DecisionTreeClassifier(random_state=0),SVC(kernel='linear',gamma=Gamma),MultinomialNB(),KNeighborsClassifier(n_neighbors=7)]\n",
        "\n",
        "\tparam = [lr_parameters[x],rbf_parameters[x],dt_parameters[x],linear_parameters[x],nb_parameters[x],knn_parameters[x]]\n",
        "\n",
        "\tfor i in range(len(classifiers)):\n",
        "\t\tp = param[i]\n",
        "\t\tprint(p)\n",
        "\t\tnum = p[0]\n",
        "\t\tsamples = p[1]\n",
        "\t\tacc,f1,cm = bagging_classify_sentiment(classifiers[i],chi_train_corpus_tf_idf,chi_test_corpus_tf_idf,label_train,label_test,num,samples)\n",
        "\n",
        "\n",
        "\t\t\n",
        "\t\tprint(\"Bagging \"+classify[i]+\" \"+\"F1 score is :\",f1)\n",
        "\t\tprint(\"Bagging \"+classify[i]+\" \"+\"confusion matrix is:\")\n",
        "\t\tprint(cm)\n",
        "\t\tprint(\"\\n\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZIPX9k7tu2z"
      },
      "source": [
        "def classify_sentiment(classifier,chi_train_corpus_tf_idf,chi_test_corpus_tf_idf,label_train,label_test):\n",
        "    clf   = classifier\n",
        "    clf.fit(chi_train_corpus_tf_idf,label_train)\n",
        "    pred = clf.predict(chi_test_corpus_tf_idf)\n",
        "    accuracy = clf.score(chi_test_corpus_tf_idf,label_test)\n",
        "    cm = confusion_matrix(pred,label_test)\n",
        "    f1 = f1_score(pred,label_test)\n",
        "    return accuracy,f1,cm "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddd9_DGrxKGt"
      },
      "source": [
        "def classifier_train(chi_train_corpus_tf_idf,label_train,chi_test_corpus_tf_idf,label_test,train_choice,test_choice):\n",
        "\n",
        "    rbf_parameters = [[0.9],[0.9],[0.9],[0.9],[0.9],[0.8],[0.9],[0.9],[0.8],[0.9],[0.9],[0.9]]\n",
        "\n",
        "    val = (train_choice)*3 + test_choice\n",
        "\n",
        "    Gamma = rbf_parameters[val][0]\n",
        "\n",
        "    classifiers = [LogisticRegression(random_state=0),SVC(gamma=Gamma),DecisionTreeClassifier(random_state=0),\n",
        "                   SVC(kernel='linear',gamma=Gamma),MultinomialNB(),KNeighborsClassifier(n_neighbors=7)]\n",
        "\n",
        "    \n",
        "    accu = []\n",
        "    \n",
        "    classify = [\"LR\",\"SVM-RBF\",\"DT\",\"SVM-L\",\"MNB\",\"KNN\"]\n",
        "\n",
        "    for i in range(len(classifiers)):\n",
        "        acc,f1,cm = classify_sentiment(classifiers[i],chi_train_corpus_tf_idf,chi_test_corpus_tf_idf,label_train,label_test)\n",
        "\n",
        "        accu.append(acc)\n",
        "\n",
        "        print(classify[i]+\" \"+\"F1 score is :\",f1)\n",
        "        print(classify[i]+\" \"+\"confusion matrix is:\")\n",
        "        print(cm)\n",
        "        print(\"\\n\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFwE1Jg5tu0C"
      },
      "source": [
        "def getvalue(val):\n",
        "\tif(val==\"books\"):\n",
        "\t\treturn 0\n",
        "\telif(val==\"dvd\"):\n",
        "\t\treturn 1\n",
        "\telif(val==\"electronics\"):\n",
        "\t\treturn 2\n",
        "\telse:\n",
        "\t\treturn 3"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T7Pc4ekxSlx"
      },
      "source": [
        "def getchoices(train,test):\n",
        "\n",
        "\tif(train==\"books\"):\n",
        "\t\ttrain_choice = 0\n",
        "\t\tif(test==\"dvd\"):\n",
        "\t\t\ttest_choice = 0\n",
        "\t\telif(test==\"electronics\"):\n",
        "\t\t\ttest_choice = 1\n",
        "\t\telse:\n",
        "\t\t\ttest_choice = 2\n",
        "\n",
        "\tif(train==\"dvd\"):\n",
        "\t\ttrain_choice = 1\n",
        "\t\tif(test==\"books\"):\n",
        "\t\t\ttest_choice = 0\n",
        "\t\telif(test==\"electronics\"):\n",
        "\t\t\ttest_choice = 1\n",
        "\t\telse:\n",
        "\t\t\ttest_choice = 2\n",
        "\n",
        "\tif(train==\"electronics\"):\n",
        "\t\ttrain_choice = 2\n",
        "\t\tif(test==\"books\"):\n",
        "\t\t\ttest_choice = 0\n",
        "\t\telif(test==\"dvd\"):\n",
        "\t\t\ttest_choice = 1\n",
        "\t\telse:\n",
        "\t\t\ttest_choice = 2\n",
        "\n",
        "\tif(train==\"kitchen\"):\n",
        "\t\ttrain_choice = 3\n",
        "\t\tif(test==\"books\"):\n",
        "\t\t\ttest_choice = 0\n",
        "\t\telif(test==\"dvd\"):\n",
        "\t\t\ttest_choice = 1\n",
        "\t\telse:\n",
        "\t\t\ttest_choice = 2\n",
        "\n",
        "\treturn train_choice,test_choice"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMgdYxEhtuxS"
      },
      "source": [
        "original_dir = \"Dataset/sorted_data_acl\"\n",
        "\n",
        "data_list = [\"/books\",\"/dvd\",\"/electronics\",\"/kitchen_and_housewares\"]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoX25Ctdxdtp"
      },
      "source": [
        "for i in range(4):\n",
        "\tdir_path = original_dir+data_list[i]\n",
        "\n",
        "\t\n",
        "\tpos_file = dir_path+\"/positive.review\"\n",
        "\tneg_file = dir_path+\"/negative.review\"\n",
        "\n",
        "\textract_files(pos_file,neg_file,i)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g4sn_dEtuuj",
        "outputId": "b4dd0677-28d7-4539-8e76-a98f1dd49728"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\ttrain = input(\"Enter dataset name to train: \")\n",
        "\n",
        "\ttest = input(\"Enter dataset name to test: \")\n",
        "\n",
        "\t\n",
        "\ttrain_choice,test_choice = getchoices(train,test)\n",
        "\n",
        "\tpre_choice_train = getvalue(train)\n",
        "\tpre_choice_test = getvalue(test)\n",
        "\n",
        "\tcorpus_train,label_train,corpus_test,label_test = preprocessing(pre_choice_train,pre_choice_test)\n",
        "\n",
        "\t\n",
        "\ttrain_corpus_tf_idf,test_corpus_tf_idf = featureextraction(corpus_train,corpus_test,label_train,train_choice,test_choice)\n",
        "\n",
        "\tchi_train_corpus_tf_idf,chi_test_corpus_tf_idf = featureselection(train_corpus_tf_idf,test_corpus_tf_idf,label_train,train_choice,test_choice)\n",
        "\n",
        "\tclassifier_train(chi_train_corpus_tf_idf,label_train,chi_test_corpus_tf_idf,label_test,train_choice,test_choice)\n",
        "\n",
        "\tbagging_train(chi_train_corpus_tf_idf,label_train,chi_test_corpus_tf_idf,label_test,train_choice,test_choice)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter dataset name to train: books\n",
            "Enter dataset name to test: kitchen\n",
            "LR F1 score is : 0.7281553398058253\n",
            "LR confusion matrix is:\n",
            "[[138  50]\n",
            " [ 62 150]]\n",
            "\n",
            "\n",
            "SVM-RBF F1 score is : 0.7245657568238213\n",
            "SVM-RBF confusion matrix is:\n",
            "[[143  54]\n",
            " [ 57 146]]\n",
            "\n",
            "\n",
            "DT F1 score is : 0.624\n",
            "DT confusion matrix is:\n",
            "[[142  83]\n",
            " [ 58 117]]\n",
            "\n",
            "\n",
            "SVM-L F1 score is : 0.6835443037974683\n",
            "SVM-L confusion matrix is:\n",
            "[[140  65]\n",
            " [ 60 135]]\n",
            "\n",
            "\n",
            "MNB F1 score is : 0.6852791878172588\n",
            "MNB confusion matrix is:\n",
            "[[141  65]\n",
            " [ 59 135]]\n",
            "\n",
            "\n",
            "KNN F1 score is : 0.6245210727969349\n",
            "KNN confusion matrix is:\n",
            "[[ 41  37]\n",
            " [159 163]]\n",
            "\n",
            "\n",
            "[6, 0.5]\n",
            "Bagging LR F1 score is : 0.7380952380952381\n",
            "Bagging LR confusion matrix is:\n",
            "[[135  45]\n",
            " [ 65 155]]\n",
            "\n",
            "\n",
            "[13, 0.7]\n",
            "Bagging SVM-RBF F1 score is : 0.7164179104477613\n",
            "Bagging SVM-RBF confusion matrix is:\n",
            "[[142  56]\n",
            " [ 58 144]]\n",
            "\n",
            "\n",
            "[15, 0.5]\n",
            "Bagging DT F1 score is : 0.690807799442897\n",
            "Bagging DT confusion matrix is:\n",
            "[[165  76]\n",
            " [ 35 124]]\n",
            "\n",
            "\n",
            "[7, 0.9]\n",
            "Bagging SVM-L F1 score is : 0.725925925925926\n",
            "Bagging SVM-L confusion matrix is:\n",
            "[[142  53]\n",
            " [ 58 147]]\n",
            "\n",
            "\n",
            "[5, 0.7]\n",
            "Bagging MNB F1 score is : 0.7067307692307692\n",
            "Bagging MNB confusion matrix is:\n",
            "[[131  53]\n",
            " [ 69 147]]\n",
            "\n",
            "\n",
            "[7, 0.5]\n",
            "Bagging KNN F1 score is : 0.6608084358523726\n",
            "Bagging KNN confusion matrix is:\n",
            "[[ 19  12]\n",
            " [181 188]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}